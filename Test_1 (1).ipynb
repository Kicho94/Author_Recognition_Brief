{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tDTLQBNuSbxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "412ewHWRnyvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_text = '/content/drive/MyDrive/datasets-spam.csv'\n",
        "#path_to_text = 'datasets-spam.csv'\n",
        "data = pd.read_csv(path_to_text, encoding='latin-1')[['v1', 'v2']]\n",
        "\n",
        "# Creating the feature set and label set\n",
        "text = data['v2']\n",
        "label = data['v1']\n",
        "data[5:10]"
      ],
      "metadata": {
        "id": "cPF1fCYNs-lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прво целосно прочистен"
      ],
      "metadata": {
        "id": "IkzbchTWXMEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "nltk.download('stopwords')\n",
        "stopwords_ = stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "import string"
      ],
      "metadata": {
        "id": "kQMEPwLuVos_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessed = []\n",
        "for sentence in text:\n",
        "    #sentence lower\n",
        "    sentence = sentence.lower()\n",
        "    #string punct\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    #tokenize\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    # stop-words\n",
        "    tokens_stop_words = []\n",
        "    for token in tokens:\n",
        "      if token not in stopwords_:\n",
        "        tokens_stop_words.append(token)\n",
        "    # Lemmatization\n",
        "    tokens_lemma = []\n",
        "    for token in tokens_stop_words:\n",
        "      tokens_lemma.append(wnl.lemmatize(token, get_wordnet_pos(nltk.pos_tag([token])[0][1])))\n",
        "    final = ' '.join(tokens_lemma)\n",
        "    text_preprocessed.append(final)"
      ],
      "metadata": {
        "id": "G4TuGNFyVFld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_preprocessed),len(text)"
      ],
      "metadata": {
        "id": "MEIDYJ-dtkWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Без процесирање, само мали букви и токенизација."
      ],
      "metadata": {
        "id": "UCSyKfMSXUV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessed_1 = []\n",
        "for sentence in text:\n",
        "    #sentence lower\n",
        "    sentence = sentence.lower()\n",
        "    #tokenize\n",
        "    tokens = nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "S1lVUTtWS2sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_preprocessed_1),len(text_preprocessed), len(text)"
      ],
      "metadata": {
        "id": "ToIgCuOZtlb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Vecotrizer model"
      ],
      "metadata": {
        "id": "Yf-0K8WSt-Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = .7\n",
        "train_index= int(len(text)*train_ratio)\n",
        "\n",
        "trainX, testX =text_preprocessed[:train_index], text_preprocessed[train_index:]\n",
        "trainY, testY =label[:train_index], label[train_index:]\n",
        "\n",
        "len(trainX), len(testX) ,len(trainY), len(testY)"
      ],
      "metadata": {
        "id": "qV50_5d7t9Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode lables (0-ham, 1-spam)\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "trainY= le.fit_transform(trainY)\n",
        "testY = le.fit_transform(testY)\n",
        "trainY.shape,testY.shape\n",
        "trainY[0]"
      ],
      "metadata": {
        "id": "qRaf0P1JuHBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Count Vectors as features\n",
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(max_features=5000)\n",
        "count_vect.fit(text_preprocessed)\n",
        "\n",
        "# transform the training and test data using count vectorizer object\n",
        "trainX_vec = count_vect.transform(trainX)\n",
        "testX_vec = count_vect.transform(testX)\n",
        "trainX_vec.shape,\n",
        "testX_vec.shape"
      ],
      "metadata": {
        "id": "g8Y0uHPguNI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "\n",
        "#create an instance of the model\n",
        "lr_model = LogisticRegression(random_state=7)\n",
        "#train the model\n",
        "lr_model.fit(trainX_vec, trainY)\n",
        "\n",
        "#predict test data\n",
        "pred_test = lr_model.predict(testX_vec)\n",
        "\n",
        "#print evaluation metrics \n",
        "print(classification_report(testY,pred_test))\n",
        "print(confusion_matrix(testY,pred_test))\n",
        "print(\"Accuracy:\",accuracy_score(testY, pred_test))"
      ],
      "metadata": {
        "id": "h2wb9TqQuYZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вториот модел"
      ],
      "metadata": {
        "id": "O4KSr1-hn4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_1_ratio = .7\n",
        "train_1_index= int(len(text)*train_1_ratio)\n",
        "\n",
        "trainX_1, testX_1 =text_preprocessed_1[:train_index], text_preprocessed_1[train_index:]\n",
        "trainY_1, testY_1 =label[:train_1_index], label[train_1_index:]\n",
        "\n",
        "len(trainX_1), len(testX_1) ,len(trainY_1), len(testY_1)"
      ],
      "metadata": {
        "id": "TxSTEdkbx9sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "trainY_1= le.fit_transform(trainY_1)\n",
        "testY_1 = le.fit_transform(testY_1)\n",
        "trainY_1.shape,testY_1.shape\n",
        "trainY_1[0]"
      ],
      "metadata": {
        "id": "WO-aOegA1S4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect_1 = CountVectorizer(max_features=5000)\n",
        "count_vect_1.fit(text_preprocessed_1)\n",
        "\n",
        "# transform the training and test data using count vectorizer object\n",
        "trainX_1_vec = count_vect.transform(trainX_1)\n",
        "testX_1_vec = count_vect.transform(testX_1)\n",
        "trainX_1_vec.shape,\n",
        "testX_1_vec.shape"
      ],
      "metadata": {
        "id": "CT0m5SOw1sOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model_1 = LogisticRegression(random_state=0)\n",
        "lr_model_1.fit(trainX_1_vec, trainY_1)\n",
        "\n",
        "pred_test_1 = lr_model_1.predict(testX_1_vec)\n",
        "\n",
        "#print evaluation metrics \n",
        "print(classification_report(testY_1,pred_test_1))\n",
        "print(confusion_matrix(testY_1,pred_test_1))\n",
        "print(\"Accuracy:\",accuracy_score(testY_1, pred_test_1))"
      ],
      "metadata": {
        "id": "T1bUVcdYo-iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "mYdUrcYmS5wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000, stop_words={‘english’}, ngram_range=(1, 2)) \n",
        "tfidf.fit(text)\n",
        "\n",
        "X_train_tfidf = tfidf.transform(trainX)\n",
        "X_test_tfidf = tfidf.transform(testX)\n",
        "\n",
        "X_train_tfidf.shape, X_test_tfidf.shape"
      ],
      "metadata": {
        "id": "IDLweyMdqpwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model_tf = LogisticRegression(random_state=0)\n",
        "lr_model_tf.fit(X_train_tfidf, trainY)\n",
        "pred_test_tf = lr_model_tf.predict(X_test_tfidf)\n",
        "\n",
        "#print evaluation metrics \n",
        "print(classification_report(testY,pred_test_tf))\n",
        "print(confusion_matrix(testY,pred_test_tf))\n",
        "print(\"Accuracy:\",accuracy_score(testY, pred_test_tf))"
      ],
      "metadata": {
        "id": "RfPZHh8lxw1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model_tf = LogisticRegression(random_state=0)\n",
        "lr_model_tf.fit(X_train_tfidf, trainY)\n",
        "pred_test_tf = lr_model_tf.predict(X_test_tfidf)\n",
        "\n",
        "#print evaluation metrics \n",
        "print(classification_report(testY,pred_test_tf))\n",
        "print(confusion_matrix(testY,pred_test_tf))\n",
        "print(\"Accuracy:\",accuracy_score(testY, pred_test_tf))"
      ],
      "metadata": {
        "id": "xx9-wnpTrHeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Language processing\n",
        "text.concordance(\"word\") - колку пати се појавува зборот\n"
      ],
      "metadata": {
        "id": "z71kI5VOo3dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mutlinomial NB, Random Forest, SVM(kernel='linear', probability=True) bi gi probala\n"
      ],
      "metadata": {
        "id": "w1FzP0f1S9N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings \n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import Phrases\n",
        "\n",
        "bigramer = Phrases(sentences)\n",
        "model = Word2Vec(bigramer[sentences], size=100, window=5, min_count=10, workers=4)\n",
        "\n",
        "# unload memory\n",
        "model.init_sims(replace=True) \n",
        "\n",
        "# Storing a model\n",
        "model.save(\"author\")\n",
        "# new_model = gensim.models.Word2Vec.load('author')\n",
        "\n",
        "# Switch to KeyedVectors instance  \n",
        "w2v = {w: vec for w,vec in zip(model.wv.indx2word, model.wv.syn0)}"
      ],
      "metadata": {
        "id": "HzUSRQfOZylr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar()"
      ],
      "metadata": {
        "id": "dvXe3ijQehQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingVectorizer(object):\n",
        "    # If word2vec were passed in during initialization, use those\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = 100\n",
        "    \n",
        "    # learning word2weight\n",
        "    def fit(self, X, y):\n",
        "        vect = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
        "        vect.fit(X)\n",
        "        max_idf = max(vect.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "        lambda: max_idf, [(w, vect.idf_[i]) for w, i in vect.vocabulary_.items()]\n",
        "        )\n",
        "        return self\n",
        "    \n",
        "    # Use learned word2weight\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([\n",
        "                self.word2vec[w]*self.word2weight[w] \n",
        "                for w in words if w in self.word2vec] or \n",
        "                [np.zeros(self.dim)], axis=0) \n",
        "            for words in X\n",
        "        ])\n",
        "        "
      ],
      "metadata": {
        "id": "nae2IGVjicsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer #similar to the CountVectorizer and TfIDF from sci-kit\n",
        "\n",
        "#The word embedding layer expects input sequences to be comprised of integers.\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(text_preprocessed)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text_preprocessed)"
      ],
      "metadata": {
        "id": "8QH12lzY_W_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequences[])"
      ],
      "metadata": {
        "id": "0NcZoThSAAR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "Y3ES4MU5_0jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)\n",
        "\n",
        "train_part= int(len(text)*0.7)\n",
        "train_X, test_X =sequences[:train_part], sequences[train_part:]\n",
        "\n",
        "train_X.shape,test_X.shape"
      ],
      "metadata": {
        "id": "hTxuijiE_-yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 30\n",
        "#transforms a list (of length num_samples) of sequences (lists of integers) \n",
        "#into a 2D Numpy array of shape (num_samples, num_timesteps) num_timesteps is the maxlen argument.\n",
        "train_X_pad = pad_sequences(train_X, maxlen=max_len)\n",
        "test_X_pad = pad_sequences(test_X, maxlen=max_len)\n",
        "print(train_X_pad.shape)\n",
        "print(test_X_pad.shape)"
      ],
      "metadata": {
        "id": "E3ykWyt8AQHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "ePA_jmq1Ah-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input,Embedding,LSTM,Dense\n",
        "\n",
        "def generate_model(vocab_size, max_len, embedding_size):\n",
        "\n",
        "  _input = Input(max_len)\n",
        "\n",
        "  x = Embedding(input_dim = vocab_size, output_dim = embedding_size) (_input)\n",
        "\n",
        "  x = LSTM(100)(x)\n",
        "\n",
        "  output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs= [_input], outputs = [output])\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "luSRMsQ7AX-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = generate_model(vocab_size,max_len,embedding_size=50)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "J3WapLTSAcxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_X_pad, trainY, epochs=10, batch_size=60)"
      ],
      "metadata": {
        "id": "P9xrX7gwAmZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = model.predict(test_X_pad)\n",
        "#pred_test = np.argmax(pred_test,axis=1)\n",
        "pred_test = pred_test.round()\n",
        "\n",
        "#print evaluation metrics \n",
        "print(classification_report(testY,pred_test))\n",
        "print(confusion_matrix(testY,pred_test))\n",
        "print(\"Accuracy:\",accuracy_score(testY, pred_test))"
      ],
      "metadata": {
        "id": "wMmTww8hAqC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}