{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tDTLQBNuSbxU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPF1fCYNs-lK",
    "outputId": "0f1cdbf2-6ed5-4f9a-97e4-6e3898595c05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Pretty soon I wanted to smoke, and asked the w...\n",
       "1      Her sister, Miss Watson, a tolerable slim old ...\n",
       "2      Now she had got a start, and she went on and t...\n",
       "3      Miss Watson she kept pecking at me, and it got...\n",
       "4      I set down again, a-shaking all over, and got ...\n",
       "                             ...                        \n",
       "994    I was on the point of asking him what that wor...\n",
       "995      1. Knowledge of Literature.--Nil.\\r\\n  2.   ...\n",
       "996    I see that I have alluded above to his powers ...\n",
       "997    During the first week or so we had no callers,...\n",
       "998    It was upon the 4th of March, as I have good r...\n",
       "Name: v2, Length: 999, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_text = '/content/drive/MyDrive/datasets-spam.csv'\n",
    "#path_to_text = 'datasets-spam.csv'\n",
    "path_to_text = 'books_and_authors'\n",
    "data = pd.read_csv(path_to_text, names=['v1', 'v2'])\n",
    "\n",
    "# Creating the feature set and label set\n",
    "label = data['v1']\n",
    "text = data['v2']\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HM5b5MXpjwu"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzbchTWXMEQ"
   },
   "source": [
    "### Прво целосно прочистен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQMEPwLuVos_",
    "outputId": "15d9dd77-2d15-46d7-db17-5f494261b260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G4TuGNFyVFld"
   },
   "outputs": [],
   "source": [
    "text_preprocessed = []\n",
    "for sentence in text:\n",
    "    #sentence lower\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #string punct\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # stop-words\n",
    "    tokens_stop_words = []\n",
    "    \n",
    "    for token in tokens:\n",
    "          if token not in stopwords_:\n",
    "                tokens_stop_words.append(token)\n",
    "                \n",
    "    # Lemmatization\n",
    "    tokens_lemma = []\n",
    "    for token in tokens_stop_words:\n",
    "          tokens_lemma.append(wnl.lemmatize(token, get_wordnet_pos(nltk.pos_tag([token])[0][1])))\n",
    "            \n",
    "    final = ' '.join(tokens_lemma)\n",
    "    \n",
    "    text_preprocessed.append(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCSyKfMSXUV-"
   },
   "source": [
    "Без процесирање, само мали букви и токенизација."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S1lVUTtWS2sW"
   },
   "outputs": [],
   "source": [
    "text_preprocessed_1 = []\n",
    "for sentence in text:\n",
    "    #sentence lower\n",
    "    sentence = sentence.lower()\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    text_preprocessed_1.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToIgCuOZtlb8",
    "outputId": "6dbf165a-706c-461b-92f8-7e2ab42ee426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 999, 999)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_preprocessed_1),len(text_preprocessed), len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf-0K8WSt-Vq"
   },
   "source": [
    "### Extracting Text and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV50_5d7t9Pe",
    "outputId": "8bb9fe2d-6a4d-42f5-c5a3-a515af58b500"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    text_preprocessed, label, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRaf0P1JuHBa",
    "outputId": "e06aa909-f53c-40b0-b407-51b479a049de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode lables (0-ham, 1-spam)\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "trainY= le.fit_transform(trainY)\n",
    "testY = le.fit_transform(testY)\n",
    "trainY.shape,testY.shape\n",
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROCu1wnCpXiQ",
    "outputId": "46bb7fd9-cb04-489f-e932-b5d89064d843"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]), array([ 68, 104,  89, 167,  74, 108,  89]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trainY, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlRHzeJspXiR",
    "outputId": "e616dcc5-65c5-4c34-8698-1a897bb69e61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 1, 3, 5, 4, 5, 0, 0, 3, 5, 5, 3, 5, 1, 2, 4, 3, 2, 6, 2, 4,\n",
       "       4, 3, 0, 0, 1, 0, 2, 5, 4, 5, 1, 6, 4, 3, 3, 4, 5, 1, 1, 2, 6, 5,\n",
       "       3, 0, 6, 0, 2, 4, 3, 6, 4, 2, 4, 1, 4, 2, 5, 5, 2, 1, 2, 6, 0, 2,\n",
       "       3, 5, 2, 1, 2, 2, 6, 0, 3, 6, 3, 1, 2, 2, 3, 2, 2, 5, 3, 2, 0, 1,\n",
       "       2, 6, 3, 6, 5, 6, 3, 1, 5, 3, 6, 3, 1, 5, 5, 3, 0, 4, 6, 5, 5, 0,\n",
       "       5, 4, 5, 3, 3, 1, 5, 1, 6, 3, 1, 5, 5, 4, 6, 6, 0, 4, 4, 4, 4, 0,\n",
       "       4, 6, 2, 4, 1, 0, 2, 3, 0, 1, 5, 5, 3, 2, 5, 5, 5, 5, 1, 2, 2, 5,\n",
       "       3, 5, 1, 1, 6, 6, 5, 5, 1, 6, 6, 1, 3, 1, 1, 6, 3, 6, 4, 0, 0, 0,\n",
       "       5, 3, 3, 6, 6, 1, 5, 6, 6, 4, 2, 1, 0, 5, 3, 6, 1, 2, 1, 1, 0, 3,\n",
       "       5, 2, 1, 3, 3, 0, 3, 1, 0, 5, 4, 5, 4, 5, 6, 4, 3, 2, 2, 0, 6, 4,\n",
       "       5, 0, 4, 1, 0, 3, 2, 5, 6, 5, 6, 5, 4, 3, 1, 2, 6, 3, 1, 2, 4, 2,\n",
       "       1, 6, 3, 1, 5, 5, 6, 6, 1, 3, 6, 3, 0, 4, 4, 6, 4, 2, 0, 3, 3, 4,\n",
       "       1, 3, 4, 4, 0, 2, 6, 3, 6, 4, 0, 2, 1, 1, 3, 4, 6, 3, 3, 3, 1, 5,\n",
       "       3, 6, 2, 2, 3, 5, 3, 3, 2, 4, 5, 5, 3, 6, 6, 4, 4, 5, 0, 2, 6, 5,\n",
       "       5, 2, 4, 3, 3, 3, 3, 5, 5, 0, 6, 0, 2, 3, 2, 6, 1, 6, 0, 3, 5, 3,\n",
       "       1, 4, 3, 5, 2, 6, 3, 1, 0, 4, 2, 6, 0, 1, 1, 0, 3, 3, 3, 2, 3, 3,\n",
       "       1, 0, 3, 5, 0, 2, 3, 2, 3, 0, 3, 6, 2, 3, 6, 1, 1, 3, 5, 3, 3, 6,\n",
       "       6, 2, 1, 0, 2, 5, 3, 3, 6, 3, 6, 1, 5, 4, 4, 3, 5, 5, 3, 3, 4, 5,\n",
       "       4, 1, 2, 3, 5, 3, 5, 3, 1, 2, 2, 3, 0, 3, 5, 4, 1, 4, 2, 6, 3, 5,\n",
       "       2, 0, 2, 5, 4, 4, 1, 6, 2, 2, 1, 2, 1, 0, 3, 0, 3, 3, 6, 5, 1, 3,\n",
       "       0, 3, 0, 2, 5, 0, 3, 0, 0, 5, 3, 3, 1, 0, 1, 4, 0, 1, 1, 6, 3, 2,\n",
       "       3, 5, 3, 1, 0, 3, 2, 3, 3, 4, 5, 1, 0, 3, 4, 4, 3, 5, 5, 4, 1, 1,\n",
       "       2, 0, 3, 1, 4, 1, 5, 0, 0, 0, 5, 3, 3, 5, 3, 5, 0, 5, 3, 6, 2, 3,\n",
       "       2, 5, 6, 5, 2, 5, 3, 3, 3, 0, 3, 6, 5, 2, 3, 5, 3, 1, 5, 3, 2, 1,\n",
       "       3, 1, 3, 3, 2, 6, 3, 5, 3, 5, 3, 5, 3, 3, 6, 3, 3, 5, 1, 1, 6, 6,\n",
       "       3, 3, 2, 2, 6, 3, 2, 1, 3, 1, 4, 1, 3, 1, 5, 1, 2, 6, 1, 5, 6, 0,\n",
       "       5, 3, 0, 6, 3, 6, 2, 6, 1, 1, 2, 3, 2, 3, 6, 3, 4, 3, 3, 3, 4, 3,\n",
       "       4, 3, 1, 1, 0, 3, 3, 4, 2, 2, 5, 3, 1, 3, 1, 3, 3, 6, 5, 1, 6, 0,\n",
       "       0, 5, 6, 2, 6, 3, 1, 1, 1, 4, 6, 4, 3, 5, 4, 3, 2, 6, 4, 5, 1, 4,\n",
       "       2, 3, 0, 5, 3, 1, 0, 1, 3, 2, 5, 3, 5, 3, 2, 3, 3, 1, 3, 1, 6, 1,\n",
       "       5, 3, 3, 4, 6, 4, 1, 2, 4, 0, 2, 1, 3, 1, 1, 4, 3, 4, 5, 2, 5, 3,\n",
       "       6, 1, 6, 1, 4, 2, 1, 6, 5, 6, 3, 6, 6, 4, 3, 1, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8Y0uHPguNI1",
    "outputId": "eff04cc4-52d5-4d9e-e53d-0842ea6f0fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectors as features\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(max_features=5000)\n",
    "count_vect.fit(text_preprocessed)\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "trainX_vec = count_vect.transform(trainX)\n",
    "testX_vec = count_vect.transform(testX)\n",
    "trainX_vec.shape,\n",
    "testX_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2wb9TqQuYZE",
    "outputId": "92f2a195-d1f7-43b3-f461-9d4b2fb17bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83        30\n",
      "           1       0.83      0.87      0.85        39\n",
      "           2       0.70      0.74      0.72        31\n",
      "           3       0.83      0.95      0.89        65\n",
      "           4       0.93      0.87      0.90        46\n",
      "           5       0.76      0.71      0.73        48\n",
      "           6       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.83      0.81      0.82       300\n",
      "weighted avg       0.83      0.83      0.83       300\n",
      "\n",
      "[[24  1  4  0  0  1  0]\n",
      " [ 1 34  0  3  1  0  0]\n",
      " [ 1  1 23  1  0  5  0]\n",
      " [ 0  2  0 62  0  1  0]\n",
      " [ 1  0  0  2 40  0  3]\n",
      " [ 1  1  5  5  1 34  1]\n",
      " [ 0  2  1  2  1  4 31]]\n",
      "Accuracy: 0.8266666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "#create an instance of the model\n",
    "lr_model = LogisticRegression(random_state=7)\n",
    "#train the model\n",
    "lr_model.fit(trainX_vec, trainY)\n",
    "\n",
    "#predict test data\n",
    "pred_test = lr_model.predict(testX_vec)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test))\n",
    "print(confusion_matrix(testY,pred_test))\n",
    "print(\"Accuracy:\", accuracy_score(testY, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4KSr1-hn4aY"
   },
   "source": [
    "### Вториот модел - не процесиран текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxSTEdkbx9sZ",
    "outputId": "264e7a10-205b-4210-a9c3-5d670be8710b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX_1, testX_1, trainY_1, testY_1 = train_test_split(\n",
    "    text_preprocessed_1, label, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)\n",
    "\n",
    "\n",
    "len(trainX_1), len(testX_1) ,len(trainY_1), len(testY_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WO-aOegA1S4k",
    "outputId": "f9dd882d-6cdf-4fca-b013-1824594db0f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "trainY_1= le.fit_transform(trainY_1)\n",
    "testY_1 = le.fit_transform(testY_1)\n",
    "trainY_1.shape,testY_1.shape\n",
    "trainY_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CT0m5SOw1sOL",
    "outputId": "77c76225-af32-4c79-f642-362442633378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_1 = CountVectorizer(max_features=5000)\n",
    "count_vect_1.fit(text_preprocessed_1)\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "trainX_1_vec = count_vect.transform(trainX_1)\n",
    "testX_1_vec = count_vect.transform(testX_1)\n",
    "trainX_1_vec.shape,\n",
    "testX_1_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1bUVcdYo-iF",
    "outputId": "734cca50-1a5d-4c13-f710-f1ee099dce58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        30\n",
      "           1       0.72      0.85      0.78        39\n",
      "           2       0.69      0.71      0.70        31\n",
      "           3       0.80      0.92      0.86        65\n",
      "           4       0.81      0.76      0.79        46\n",
      "           5       0.88      0.73      0.80        48\n",
      "           6       0.79      0.66      0.72        41\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.78      0.78      0.78       300\n",
      "weighted avg       0.79      0.79      0.79       300\n",
      "\n",
      "[[24  1  3  1  1  0  0]\n",
      " [ 1 33  0  0  2  2  1]\n",
      " [ 2  2 22  2  1  1  1]\n",
      " [ 0  2  1 60  1  1  0]\n",
      " [ 1  3  0  2 35  0  5]\n",
      " [ 0  2  4  6  1 35  0]\n",
      " [ 2  3  2  4  2  1 27]]\n",
      "Accuracy: 0.7866666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=0)\n",
    "lr_model_1.fit(trainX_1_vec, trainY_1)\n",
    "\n",
    "pred_test_1 = lr_model_1.predict(testX_1_vec)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY_1,pred_test_1))\n",
    "print(confusion_matrix(testY_1,pred_test_1))\n",
    "print(\"Accuracy:\",accuracy_score(testY_1, pred_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mYdUrcYmS5wt"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDLweyMdqpwX",
    "outputId": "ff2b14e5-15a1-460f-8406-c8ce603fa6c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((699, 5000), (300, 5000))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, stop_words={\"english\"}, ngram_range=(1, 3)) \n",
    "tfidf.fit(text)\n",
    "\n",
    "X_train_tfidf = tfidf.transform(trainX)\n",
    "X_test_tfidf = tfidf.transform(testX)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfPZHh8lxw1z",
    "outputId": "03aa567c-a9e0-4a37-811c-62c36f1e24a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        30\n",
      "           1       0.73      0.90      0.80        39\n",
      "           2       0.82      0.74      0.78        31\n",
      "           3       0.72      0.97      0.83        65\n",
      "           4       0.95      0.85      0.90        46\n",
      "           5       0.89      0.83      0.86        48\n",
      "           6       0.81      0.71      0.75        41\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.85      0.79      0.80       300\n",
      "weighted avg       0.83      0.81      0.81       300\n",
      "\n",
      "[[15  4  2  7  0  2  0]\n",
      " [ 0 35  0  1  1  0  2]\n",
      " [ 0  2 23  3  0  2  1]\n",
      " [ 0  1  0 63  0  0  1]\n",
      " [ 0  2  0  2 39  0  3]\n",
      " [ 0  0  3  5  0 40  0]\n",
      " [ 0  4  0  6  1  1 29]]\n",
      "Accuracy: 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "lr_model_tf = LogisticRegression(random_state=0)\n",
    "lr_model_tf.fit(X_train_tfidf, trainY)\n",
    "pred_test_tf = lr_model_tf.predict(X_test_tfidf)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test_tf))\n",
    "print(confusion_matrix(testY,pred_test_tf))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xx9-wnpTrHeg",
    "outputId": "3b9bbd8b-4e85-4c55-ff79-a50a626ce2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        30\n",
      "           1       0.73      0.90      0.80        39\n",
      "           2       0.82      0.74      0.78        31\n",
      "           3       0.72      0.97      0.83        65\n",
      "           4       0.95      0.85      0.90        46\n",
      "           5       0.89      0.83      0.86        48\n",
      "           6       0.81      0.71      0.75        41\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.85      0.79      0.80       300\n",
      "weighted avg       0.83      0.81      0.81       300\n",
      "\n",
      "[[15  4  2  7  0  2  0]\n",
      " [ 0 35  0  1  1  0  2]\n",
      " [ 0  2 23  3  0  2  1]\n",
      " [ 0  1  0 63  0  0  1]\n",
      " [ 0  2  0  2 39  0  3]\n",
      " [ 0  0  3  5  0 40  0]\n",
      " [ 0  4  0  6  1  1 29]]\n",
      "Accuracy: 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "lr_model_tf = LogisticRegression(random_state=0)\n",
    "lr_model_tf.fit(X_train_tfidf, trainY)\n",
    "pred_test_tf = lr_model_tf.predict(X_test_tfidf)\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test_tf))\n",
    "print(confusion_matrix(testY,pred_test_tf))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z71kI5VOo3dA",
    "outputId": "62396fc2-9a5d-4661-e052-ee1458f82a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "#Language processing\n",
    "from nltk.text import Text\n",
    "text_class = Text(text)\n",
    "text_class.concordance(\"crime\") # - колку пати се појавува зборот\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9YKtB8-pXid",
    "outputId": "a2755661-9771-4528-af4c-ee68d451f7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Now she had got a start, and she went on and told me all about the good\r\n",
      "place. She said all a body would have to do there was to go around all\r\n",
      "day long with a harp and sing, forever and ever. So I didn't think\r\n",
      "much of it. But I never said so. I asked her if she reckoned Tom Sawyer\r\n",
      "would go there, and she said not by a considerable sight. I was glad\r\n",
      "about that, because I wanted him and me to be together.\n"
     ]
    }
   ],
   "source": [
    "words= nltk.word_tokenize(text[2])\n",
    "length= len(words) \n",
    "print(length) \n",
    "print(text[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bX8a2DtpXid",
    "outputId": "29a68ed8-66fc-4c64-9e0c-857e62189efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Yo' ole father doan' know yit what he's a-gwyne to do. Sometimes he\r\n",
      "spec he'll go 'way, en den agin he spec he'll stay. De bes' way is to\r\n",
      "res' easy en let de ole man take his own way. Dey's two angels hoverin'\r\n",
      "roun' 'bout him. One uv 'em is white en shiny, en t'other one is black.\r\n",
      "De white one gits him to go right a little while, den de black one sail\r\n",
      "in en bust it all up. A body can't tell yit which one gwyne to fetch\r\n",
      "him at de las'. But you is all right. You gwyne to have considable\r\n",
      "trouble in yo' life, en considable joy. Sometimes you gwyne to git\r\n",
      "hurt, en sometimes you gwyne to git sick; but every time you's gwyne\r\n",
      "to git well agin. Dey's two gals flyin' 'bout you in yo' life. One\r\n",
      "uv 'em's light en t'other one is dark. One is rich en t'other is po'.\r\n",
      "You's gwyne to marry de po' one fust en de rich one by en by. You\r\n",
      "wants to keep 'way fum de water as much as you kin, en don't run no\r\n",
      "resk, 'kase it's down in de bills dat you's gwyne to git hung.”\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "paragraph = text[25]\n",
    "sentences = nltk.sent_tokenize(paragraph) \n",
    "length= len(sentences) \n",
    "print(text[25]) \n",
    "print(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "w1FzP0f1S9N3"
   },
   "outputs": [],
   "source": [
    "# mutlinomial NB, Random Forest, SVM(kernel='linear', probability=True) bi gi probala\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HzUSRQfOZylr"
   },
   "outputs": [],
   "source": [
    "# # embeddings \n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from gensim.models import Phrases\n",
    "\n",
    "# bigramer = Phrases(sentences)\n",
    "# model = Word2Vec(bigramer[sentences], window=5, min_count=10, workers=4)\n",
    "\n",
    "# # unload memory\n",
    "# model.init_sims(replace=True) \n",
    "\n",
    "# # Storing a model\n",
    "# model.save(\"author\")\n",
    "# # new_model = gensim.models.Word2Vec.load('author')\n",
    "\n",
    "# # Switch to KeyedVectors instance  \n",
    "# # w2v = {w: vec for w,vec in text_preprocessed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dvXe3ijQehQm"
   },
   "outputs": [],
   "source": [
    "# model.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nae2IGVjicsO"
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    # If word2vec were passed in during initialization, use those\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 100\n",
    "    \n",
    "    # learning word2weight\n",
    "    def fit(self, X, y):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
    "        vect.fit(X)\n",
    "        max_idf = max(vect.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "        lambda: max_idf, [(w, vect.idf_[i]) for w, i in vect.vocabulary_.items()]\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    # Use learned word2weight\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([\n",
    "                self.word2vec[w]*self.word2weight[w] \n",
    "                for w in words if w in self.word2vec] or \n",
    "                [np.zeros(self.dim)], axis=0) \n",
    "            for words in X\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8QH12lzY_W_Z"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer #similar to the CountVectorizer and TfIDF from sci-kit\n",
    "\n",
    "#The word embedding layer expects input sequences to be comprised of integers.\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(text_preprocessed)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3ES4MU5_0jI",
    "outputId": "59c51d57-a072-4683-a112-32940ebd1fc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'’': 1,\n",
       " '“': 2,\n",
       " '”': 3,\n",
       " 'say': 4,\n",
       " 'would': 5,\n",
       " 'one': 6,\n",
       " 'go': 7,\n",
       " 'mr': 8,\n",
       " 'make': 9,\n",
       " 'man': 10,\n",
       " 'could': 11,\n",
       " 'like': 12,\n",
       " 'come': 13,\n",
       " 'take': 14,\n",
       " 'time': 15,\n",
       " 'get': 16,\n",
       " 'see': 17,\n",
       " 'upon': 18,\n",
       " 'little': 19,\n",
       " 'know': 20,\n",
       " 'look': 21,\n",
       " 'well': 22,\n",
       " 'great': 23,\n",
       " 'hand': 24,\n",
       " 'good': 25,\n",
       " 'give': 26,\n",
       " 'much': 27,\n",
       " 'thing': 28,\n",
       " 'seem': 29,\n",
       " 'way': 30,\n",
       " 'old': 31,\n",
       " '‘': 32,\n",
       " 'might': 33,\n",
       " 'day': 34,\n",
       " 'never': 35,\n",
       " 'even': 36,\n",
       " 'two': 37,\n",
       " 'every': 38,\n",
       " 'eye': 39,\n",
       " 'men': 40,\n",
       " 'turn': 41,\n",
       " 'life': 42,\n",
       " 'head': 43,\n",
       " 'thought': 44,\n",
       " 'house': 45,\n",
       " 'sir': 46,\n",
       " 'back': 47,\n",
       " 'young': 48,\n",
       " 'people': 49,\n",
       " 'woman': 50,\n",
       " 'first': 51,\n",
       " 'work': 52,\n",
       " 'tom': 53,\n",
       " 'must': 54,\n",
       " 'think': 55,\n",
       " 'may': 56,\n",
       " 'mind': 57,\n",
       " 'always': 58,\n",
       " 'many': 59,\n",
       " 'call': 60,\n",
       " 'face': 61,\n",
       " 'saw': 62,\n",
       " 'away': 63,\n",
       " 'long': 64,\n",
       " 'though': 65,\n",
       " 'u': 66,\n",
       " 'last': 67,\n",
       " 'year': 68,\n",
       " 'without': 69,\n",
       " 'want': 70,\n",
       " 'nothing': 71,\n",
       " 'ever': 72,\n",
       " 'casaubon': 73,\n",
       " 'boy': 74,\n",
       " 'night': 75,\n",
       " 'put': 76,\n",
       " 'place': 77,\n",
       " 'world': 78,\n",
       " 'yet': 79,\n",
       " 'found': 80,\n",
       " 'lady': 81,\n",
       " 'knew': 82,\n",
       " 'right': 83,\n",
       " 'open': 84,\n",
       " 'friend': 85,\n",
       " 'enough': 86,\n",
       " 'three': 87,\n",
       " 'large': 88,\n",
       " 'still': 89,\n",
       " 'miss': 90,\n",
       " 'begin': 91,\n",
       " 'point': 92,\n",
       " 'small': 93,\n",
       " 'show': 94,\n",
       " 'mother': 95,\n",
       " 'joe': 96,\n",
       " 'soon': 97,\n",
       " 'tell': 98,\n",
       " 'light': 99,\n",
       " 'end': 100,\n",
       " 'side': 101,\n",
       " 'word': 102,\n",
       " 'far': 103,\n",
       " 'water': 104,\n",
       " 'fire': 105,\n",
       " 'another': 106,\n",
       " 'quite': 107,\n",
       " 'ask': 108,\n",
       " 'use': 109,\n",
       " 'keep': 110,\n",
       " 'home': 111,\n",
       " 'part': 112,\n",
       " 'new': 113,\n",
       " 'round': 114,\n",
       " 'family': 115,\n",
       " 'child': 116,\n",
       " 'left': 117,\n",
       " 'shall': 118,\n",
       " 'dorothea': 119,\n",
       " 'dog': 120,\n",
       " 'brooke': 121,\n",
       " 'along': 122,\n",
       " 'sort': 123,\n",
       " 'however': 124,\n",
       " 'letter': 125,\n",
       " 'sister': 126,\n",
       " 'feel': 127,\n",
       " 'heart': 128,\n",
       " 'set': 129,\n",
       " 'lay': 130,\n",
       " 'do': 131,\n",
       " 'black': 132,\n",
       " 'rather': 133,\n",
       " 'kind': 134,\n",
       " 'talk': 135,\n",
       " 'poor': 136,\n",
       " 'door': 137,\n",
       " 'among': 138,\n",
       " 'love': 139,\n",
       " 'heard': 140,\n",
       " 'let': 141,\n",
       " 'find': 142,\n",
       " 'whole': 143,\n",
       " 'till': 144,\n",
       " 'high': 145,\n",
       " 'moment': 146,\n",
       " 'become': 147,\n",
       " 'something': 148,\n",
       " 'felt': 149,\n",
       " 'close': 150,\n",
       " 'walk': 151,\n",
       " 'maggie': 152,\n",
       " 'mean': 153,\n",
       " 'anything': 154,\n",
       " 'money': 155,\n",
       " 'believe': 156,\n",
       " 'interest': 157,\n",
       " 'wish': 158,\n",
       " 'whose': 159,\n",
       " 'live': 160,\n",
       " 'stand': 161,\n",
       " 'present': 162,\n",
       " 'course': 163,\n",
       " 'foot': 164,\n",
       " 'body': 165,\n",
       " 'towards': 166,\n",
       " 'air': 167,\n",
       " 'try': 168,\n",
       " 'bad': 169,\n",
       " 'next': 170,\n",
       " 'learn': 171,\n",
       " 'form': 172,\n",
       " 'hour': 173,\n",
       " 'room': 174,\n",
       " 'nature': 175,\n",
       " 'around': 176,\n",
       " 'best': 177,\n",
       " 'father': 178,\n",
       " 'carry': 179,\n",
       " 'wife': 180,\n",
       " 'told': 181,\n",
       " 'rest': 182,\n",
       " 'name': 183,\n",
       " 'behind': 184,\n",
       " 'half': 185,\n",
       " 'gentleman': 186,\n",
       " 'reason': 187,\n",
       " 'dear': 188,\n",
       " 'book': 189,\n",
       " 'celia': 190,\n",
       " 'together': 191,\n",
       " 'run': 192,\n",
       " 'minute': 193,\n",
       " 'country': 194,\n",
       " 'less': 195,\n",
       " 'manner': 196,\n",
       " 'also': 197,\n",
       " 'cold': 198,\n",
       " 'white': 199,\n",
       " 'speak': 200,\n",
       " 'perhaps': 201,\n",
       " 'dark': 202,\n",
       " 'morning': 203,\n",
       " 'omar': 204,\n",
       " 'tulliver': 205,\n",
       " 'die': 206,\n",
       " 'near': 207,\n",
       " 'certain': 208,\n",
       " 'fact': 209,\n",
       " 'hair': 210,\n",
       " 'write': 211,\n",
       " 'fine': 212,\n",
       " 'read': 213,\n",
       " 'full': 214,\n",
       " 'within': 215,\n",
       " 'idea': 216,\n",
       " 'pretty': 217,\n",
       " 'sure': 218,\n",
       " 'short': 219,\n",
       " 'appear': 220,\n",
       " 'girl': 221,\n",
       " 'brought': 222,\n",
       " 'church': 223,\n",
       " 'everything': 224,\n",
       " 'order': 225,\n",
       " 'help': 226,\n",
       " 'step': 227,\n",
       " 'sat': 228,\n",
       " 'return': 229,\n",
       " 'often': 230,\n",
       " 'poe': 231,\n",
       " 'window': 232,\n",
       " 'since': 233,\n",
       " 'follow': 234,\n",
       " 'matter': 235,\n",
       " 'held': 236,\n",
       " 'leave': 237,\n",
       " 'really': 238,\n",
       " 'indeed': 239,\n",
       " 'stood': 240,\n",
       " 'cry': 241,\n",
       " 'death': 242,\n",
       " 'leg': 243,\n",
       " 'stop': 244,\n",
       " 'arm': 245,\n",
       " 'whether': 246,\n",
       " 'person': 247,\n",
       " 'wolf': 248,\n",
       " 'knowledge': 249,\n",
       " 'several': 250,\n",
       " 'didnt': 251,\n",
       " 'mouth': 252,\n",
       " 'rise': 253,\n",
       " 'move': 254,\n",
       " 'hope': 255,\n",
       " 'almost': 256,\n",
       " 'master': 257,\n",
       " 'table': 258,\n",
       " 'sound': 259,\n",
       " 'notice': 260,\n",
       " 'mention': 261,\n",
       " 'hold': 262,\n",
       " 'understand': 263,\n",
       " 'ran': 264,\n",
       " 'remember': 265,\n",
       " 'voice': 266,\n",
       " 'character': 267,\n",
       " 'dead': 268,\n",
       " 'six': 269,\n",
       " 'king': 270,\n",
       " 'opinion': 271,\n",
       " 'wind': 272,\n",
       " 'need': 273,\n",
       " 'town': 274,\n",
       " 'hear': 275,\n",
       " 'alone': 276,\n",
       " 'beautiful': 277,\n",
       " 'line': 278,\n",
       " 'across': 279,\n",
       " 'street': 280,\n",
       " 'age': 281,\n",
       " 'regard': 282,\n",
       " 'least': 283,\n",
       " 'case': 284,\n",
       " 'english': 285,\n",
       " 'james': 286,\n",
       " 'particular': 287,\n",
       " 'start': 288,\n",
       " 'thousand': 289,\n",
       " 'five': 290,\n",
       " 'cut': 291,\n",
       " 'hardly': 292,\n",
       " 'hundred': 293,\n",
       " 'low': 294,\n",
       " 'occasion': 295,\n",
       " 'hard': 296,\n",
       " 'change': 297,\n",
       " 'kept': 298,\n",
       " 'lose': 299,\n",
       " 'ground': 300,\n",
       " 'state': 301,\n",
       " 'dress': 302,\n",
       " 'attention': 303,\n",
       " 'doubt': 304,\n",
       " 'sit': 305,\n",
       " 'view': 306,\n",
       " 'expect': 307,\n",
       " 'judge': 308,\n",
       " 'son': 309,\n",
       " 'general': 310,\n",
       " 'thus': 311,\n",
       " 'enter': 312,\n",
       " 'mere': 313,\n",
       " 'object': 314,\n",
       " 'receive': 315,\n",
       " 'silas': 316,\n",
       " 'heavy': 317,\n",
       " 'four': 318,\n",
       " 'wall': 319,\n",
       " 'sometimes': 320,\n",
       " 'drop': 321,\n",
       " 'school': 322,\n",
       " 'reach': 323,\n",
       " 'business': 324,\n",
       " 'boat': 325,\n",
       " 'brother': 326,\n",
       " 'fear': 327,\n",
       " 'question': 328,\n",
       " 'suppose': 329,\n",
       " 'send': 330,\n",
       " 'certainly': 331,\n",
       " 'couldnt': 332,\n",
       " 'spirit': 333,\n",
       " 'stone': 334,\n",
       " 'beyond': 335,\n",
       " 'remain': 336,\n",
       " 'fellow': 337,\n",
       " 'john': 338,\n",
       " 'clear': 339,\n",
       " 'stick': 340,\n",
       " 'rush': 341,\n",
       " 'lie': 342,\n",
       " 'bit': 343,\n",
       " 'laugh': 344,\n",
       " 'strong': 345,\n",
       " 'lead': 346,\n",
       " 'true': 347,\n",
       " 'forward': 348,\n",
       " 'horse': 349,\n",
       " 'already': 350,\n",
       " 'sense': 351,\n",
       " 'sea': 352,\n",
       " 'handsome': 353,\n",
       " 'dashwood': 354,\n",
       " 'marner': 355,\n",
       " 'listen': 356,\n",
       " 'big': 357,\n",
       " 'touch': 358,\n",
       " 'none': 359,\n",
       " 'front': 360,\n",
       " 'save': 361,\n",
       " 'pass': 362,\n",
       " 'effect': 363,\n",
       " 'truth': 364,\n",
       " 'expression': 365,\n",
       " 'husband': 366,\n",
       " 'trouble': 367,\n",
       " 'shoulder': 368,\n",
       " 'sign': 369,\n",
       " 'fell': 370,\n",
       " 'sleep': 371,\n",
       " 'happen': 372,\n",
       " 'eat': 373,\n",
       " 'else': 374,\n",
       " 'others': 375,\n",
       " 'bring': 376,\n",
       " 'grow': 377,\n",
       " 'possible': 378,\n",
       " 'uncle': 379,\n",
       " 'real': 380,\n",
       " 'cause': 381,\n",
       " 'ear': 382,\n",
       " 'hill': 383,\n",
       " 'charm': 384,\n",
       " 'month': 385,\n",
       " 'daughter': 386,\n",
       " 'fall': 387,\n",
       " 'watch': 388,\n",
       " 'soul': 389,\n",
       " 'neither': 390,\n",
       " 'land': 391,\n",
       " 'picture': 392,\n",
       " 'remarkable': 393,\n",
       " 'holmes': 394,\n",
       " 'company': 395,\n",
       " 'deep': 396,\n",
       " 'either': 397,\n",
       " 'met': 398,\n",
       " 'creature': 399,\n",
       " 'visit': 400,\n",
       " 'glance': 401,\n",
       " 'altogether': 402,\n",
       " 'answer': 403,\n",
       " 'minister': 404,\n",
       " 'add': 405,\n",
       " 'marriage': 406,\n",
       " 'power': 407,\n",
       " 'sight': 408,\n",
       " 'ready': 409,\n",
       " 'account': 410,\n",
       " 'offer': 411,\n",
       " 'wore': 412,\n",
       " 'god': 413,\n",
       " 'red': 414,\n",
       " 'except': 415,\n",
       " 'desire': 416,\n",
       " 'buck': 417,\n",
       " 'warnt': 418,\n",
       " 'everybody': 419,\n",
       " 'wait': 420,\n",
       " 'laid': 421,\n",
       " 'nose': 422,\n",
       " 'play': 423,\n",
       " 'wonder': 424,\n",
       " 'river': 425,\n",
       " 'throw': 426,\n",
       " 'corner': 427,\n",
       " 'peculiar': 428,\n",
       " 'wood': 429,\n",
       " 'pay': 430,\n",
       " 'strange': 431,\n",
       " 'village': 432,\n",
       " 'pull': 433,\n",
       " 'secret': 434,\n",
       " 'raise': 435,\n",
       " 'quiet': 436,\n",
       " 'direction': 437,\n",
       " 'ye': 438,\n",
       " 'early': 439,\n",
       " 'smile': 440,\n",
       " 'story': 441,\n",
       " 'appearance': 442,\n",
       " 'habit': 443,\n",
       " 'subject': 444,\n",
       " 'longer': 445,\n",
       " 'easy': 446,\n",
       " 'clothes': 447,\n",
       " 'hat': 448,\n",
       " 'narrow': 449,\n",
       " 'sent': 450,\n",
       " 'color': 451,\n",
       " 'court': 452,\n",
       " 'blue': 453,\n",
       " 'lip': 454,\n",
       " 'sun': 455,\n",
       " 'england': 456,\n",
       " 'therefore': 457,\n",
       " 'pain': 458,\n",
       " 'care': 459,\n",
       " 'beauty': 460,\n",
       " 'piece': 461,\n",
       " 'tree': 462,\n",
       " 'servant': 463,\n",
       " 'act': 464,\n",
       " 'em': 465,\n",
       " 'week': 466,\n",
       " 'chance': 467,\n",
       " 'study': 468,\n",
       " 'suffer': 469,\n",
       " 'produce': 470,\n",
       " 'spoke': 471,\n",
       " 'although': 472,\n",
       " 'toward': 473,\n",
       " 'purpose': 474,\n",
       " 'bread': 475,\n",
       " 'observe': 476,\n",
       " 'past': 477,\n",
       " 'affection': 478,\n",
       " 'chettam': 479,\n",
       " 'wouldnt': 480,\n",
       " 'knee': 481,\n",
       " 'ten': 482,\n",
       " 'seat': 483,\n",
       " 'paper': 484,\n",
       " 'fair': 485,\n",
       " 'arab': 486,\n",
       " 'sword': 487,\n",
       " 'crowd': 488,\n",
       " 'besides': 489,\n",
       " 'pleasure': 490,\n",
       " 'circle': 491,\n",
       " 'settle': 492,\n",
       " 'consider': 493,\n",
       " 'draw': 494,\n",
       " 'dream': 495,\n",
       " 'darkness': 496,\n",
       " 'struggle': 497,\n",
       " 'relation': 498,\n",
       " 'usual': 499,\n",
       " 'genius': 500,\n",
       " 'wine': 501,\n",
       " 'taste': 502,\n",
       " 'bob': 503,\n",
       " 'widow': 504,\n",
       " 'watson': 505,\n",
       " 'cross': 506,\n",
       " 'instead': 507,\n",
       " 'nearly': 508,\n",
       " 'lean': 509,\n",
       " 'square': 510,\n",
       " 'rag': 511,\n",
       " 'weather': 512,\n",
       " 'fish': 513,\n",
       " 'presently': 514,\n",
       " 'passion': 515,\n",
       " 'happy': 516,\n",
       " 'finger': 517,\n",
       " 'length': 518,\n",
       " 'tale': 519,\n",
       " 'knight': 520,\n",
       " 'fortune': 521,\n",
       " 'arthur': 522,\n",
       " 'figure': 523,\n",
       " 'society': 524,\n",
       " 'search': 525,\n",
       " 'conversation': 526,\n",
       " 'promise': 527,\n",
       " 'able': 528,\n",
       " 'trust': 529,\n",
       " 'companion': 530,\n",
       " 'cadwallader': 531,\n",
       " 'riley': 532,\n",
       " 'kill': 533,\n",
       " 'kitchen': 534,\n",
       " 'neck': 535,\n",
       " 'mine': 536,\n",
       " 'afterwards': 537,\n",
       " 'marry': 538,\n",
       " 'stay': 539,\n",
       " 'roll': 540,\n",
       " 'cabin': 541,\n",
       " 'allow': 542,\n",
       " 'speech': 543,\n",
       " 'bright': 544,\n",
       " 'struck': 545,\n",
       " 'human': 546,\n",
       " 'action': 547,\n",
       " 'service': 548,\n",
       " 'presence': 549,\n",
       " 'difficulty': 550,\n",
       " 'various': 551,\n",
       " 'express': 552,\n",
       " 'secure': 553,\n",
       " 'especially': 554,\n",
       " 'fit': 555,\n",
       " 'lord': 556,\n",
       " 'norland': 557,\n",
       " 'anybody': 558,\n",
       " 'advantage': 559,\n",
       " 'seven': 560,\n",
       " 'different': 561,\n",
       " 'teeth': 562,\n",
       " 'whatever': 563,\n",
       " 'floor': 564,\n",
       " 'oh': 565,\n",
       " 'wonderful': 566,\n",
       " 'road': 567,\n",
       " 'wild': 568,\n",
       " 'position': 569,\n",
       " 'drew': 570,\n",
       " 'late': 571,\n",
       " 'second': 572,\n",
       " 'effort': 573,\n",
       " 'increase': 574,\n",
       " 'simple': 575,\n",
       " 'merely': 576,\n",
       " 'charles': 577,\n",
       " 'feature': 578,\n",
       " 'strength': 579,\n",
       " 'snarl': 580,\n",
       " 'deck': 581,\n",
       " 'cottage': 582,\n",
       " 'fetch': 583,\n",
       " 'star': 584,\n",
       " 'jim': 585,\n",
       " 'hung': 586,\n",
       " 'edge': 587,\n",
       " 'passage': 588,\n",
       " 'mark': 589,\n",
       " 'rich': 590,\n",
       " 'soldier': 591,\n",
       " 'worth': 592,\n",
       " 'warm': 593,\n",
       " 'dinner': 594,\n",
       " 'couple': 595,\n",
       " 'cover': 596,\n",
       " 'necessary': 597,\n",
       " 'confess': 598,\n",
       " 'launcelot': 599,\n",
       " 'hall': 600,\n",
       " 'shape': 601,\n",
       " 'respect': 602,\n",
       " 'reply': 603,\n",
       " 'pair': 604,\n",
       " 'address': 605,\n",
       " 'lift': 606,\n",
       " 'note': 607,\n",
       " 'satisfaction': 608,\n",
       " 'mill': 609,\n",
       " 'stretch': 610,\n",
       " 'reckon': 611,\n",
       " 'garden': 612,\n",
       " 'comfortable': 613,\n",
       " 'top': 614,\n",
       " 'mile': 615,\n",
       " 'broad': 616,\n",
       " 'blood': 617,\n",
       " 'snow': 618,\n",
       " 'hang': 619,\n",
       " 'natural': 620,\n",
       " 'free': 621,\n",
       " 'fast': 622,\n",
       " 'strike': 623,\n",
       " 'finally': 624,\n",
       " 'pocket': 625,\n",
       " 'gate': 626,\n",
       " 'captain': 627,\n",
       " 'pleasant': 628,\n",
       " 'term': 629,\n",
       " 'affair': 630,\n",
       " 'comfort': 631,\n",
       " 'mystery': 632,\n",
       " 'hunger': 633,\n",
       " 'entirely': 634,\n",
       " 'scene': 635,\n",
       " 'later': 636,\n",
       " 'ordinary': 637,\n",
       " 'history': 638,\n",
       " 'measure': 639,\n",
       " 'slowly': 640,\n",
       " 'latter': 641,\n",
       " 'party': 642,\n",
       " 'deal': 643,\n",
       " 'william': 644,\n",
       " 'gordon': 645,\n",
       " 'future': 646,\n",
       " 'london': 647,\n",
       " 'style': 648,\n",
       " 'health': 649,\n",
       " 'grey': 650,\n",
       " 'neighbour': 651,\n",
       " 'wopsle': 652,\n",
       " 'baskerville': 653,\n",
       " 'bed': 654,\n",
       " 'chair': 655,\n",
       " 'likely': 656,\n",
       " 'yard': 657,\n",
       " 'nobody': 658,\n",
       " 'fill': 659,\n",
       " 'pas': 660,\n",
       " 'law': 661,\n",
       " 'fog': 662,\n",
       " 'fresh': 663,\n",
       " 'green': 664,\n",
       " 'difficult': 665,\n",
       " 'shot': 666,\n",
       " 'memory': 667,\n",
       " 'pause': 668,\n",
       " 'circumstance': 669,\n",
       " 'press': 670,\n",
       " 'earth': 671,\n",
       " 'pride': 672,\n",
       " 'engage': 673,\n",
       " 'experience': 674,\n",
       " 'terror': 675,\n",
       " 'duty': 676,\n",
       " 'event': 677,\n",
       " 'suddenly': 678,\n",
       " 'complete': 679,\n",
       " 'perfect': 680,\n",
       " 'enjoy': 681,\n",
       " 'poem': 682,\n",
       " 'imagination': 683,\n",
       " 'impression': 684,\n",
       " 'elinor': 685,\n",
       " 'austin': 686,\n",
       " 'quality': 687,\n",
       " 'rode': 688,\n",
       " 'pap': 689,\n",
       " 'bank': 690,\n",
       " 'ill': 691,\n",
       " 'broke': 692,\n",
       " 'knife': 693,\n",
       " 'wrong': 694,\n",
       " 'sudden': 695,\n",
       " 'instant': 696,\n",
       " 'pound': 697,\n",
       " 'threw': 698,\n",
       " 'nine': 699,\n",
       " 'plan': 700,\n",
       " 'imagine': 701,\n",
       " 'deliver': 702,\n",
       " 'curl': 703,\n",
       " 'delight': 704,\n",
       " 'verse': 705,\n",
       " 'fashion': 706,\n",
       " 'animal': 707,\n",
       " 'arrive': 708,\n",
       " 'remark': 709,\n",
       " 'force': 710,\n",
       " 'honor': 711,\n",
       " 'sprang': 712,\n",
       " 'cease': 713,\n",
       " 'prefect': 714,\n",
       " 'furniture': 715,\n",
       " 'fail': 716,\n",
       " 'instance': 717,\n",
       " 'apart': 718,\n",
       " 'concern': 719,\n",
       " 'attempt': 720,\n",
       " 'sailor': 721,\n",
       " 'kindness': 722,\n",
       " 'leap': 723,\n",
       " 'pack': 724,\n",
       " 'passenger': 725,\n",
       " 'squire': 726,\n",
       " 'duff': 727,\n",
       " 'raveloe': 728,\n",
       " 'candle': 729,\n",
       " 'noise': 730,\n",
       " 'devil': 731,\n",
       " 'grand': 732,\n",
       " 'living': 733,\n",
       " 'de': 734,\n",
       " 'spring': 735,\n",
       " 'hunt': 736,\n",
       " 'terrible': 737,\n",
       " 'probably': 738,\n",
       " 'curiosity': 739,\n",
       " 'dare': 740,\n",
       " 'flower': 741,\n",
       " 'ago': 742,\n",
       " 'accord': 743,\n",
       " 'admiration': 744,\n",
       " 'depart': 745,\n",
       " 'spot': 746,\n",
       " 'sharp': 747,\n",
       " 'evil': 748,\n",
       " 'queen': 749,\n",
       " 'faith': 750,\n",
       " 'american': 751,\n",
       " 'match': 752,\n",
       " 'reflect': 753,\n",
       " 'wide': 754,\n",
       " 'shock': 755,\n",
       " 'determine': 756,\n",
       " 'fancy': 757,\n",
       " 'continued': 758,\n",
       " 'shout': 759,\n",
       " 'condition': 760,\n",
       " 'result': 761,\n",
       " 'immediately': 762,\n",
       " 'odd': 763,\n",
       " 'poet': 764,\n",
       " 'religious': 765,\n",
       " 'possess': 766,\n",
       " 'married': 767,\n",
       " 'join': 768,\n",
       " 'rope': 769,\n",
       " 'chest': 770,\n",
       " 'brown': 771,\n",
       " 'clergyman': 772,\n",
       " 'amiable': 773,\n",
       " 'edward': 774,\n",
       " 'clean': 775,\n",
       " 'leaf': 776,\n",
       " 'tie': 777,\n",
       " 'tear': 778,\n",
       " 'inside': 779,\n",
       " 'guard': 780,\n",
       " 'breakfast': 781,\n",
       " 'quick': 782,\n",
       " 'en': 783,\n",
       " 'hurt': 784,\n",
       " 'shut': 785,\n",
       " 'breath': 786,\n",
       " 'coat': 787,\n",
       " 'win': 788,\n",
       " 'bow': 789,\n",
       " 'language': 790,\n",
       " 'heel': 791,\n",
       " 'caught': 792,\n",
       " 'outside': 793,\n",
       " 'aunt': 794,\n",
       " 'suit': 795,\n",
       " 'conduct': 796,\n",
       " 'perfectly': 797,\n",
       " 'unto': 798,\n",
       " 'feeling': 799,\n",
       " 'pity': 800,\n",
       " 'success': 801,\n",
       " 'remove': 802,\n",
       " 'sunday': 803,\n",
       " 'equal': 804,\n",
       " 'bound': 805,\n",
       " 'require': 806,\n",
       " 'stroke': 807,\n",
       " 'kay': 808,\n",
       " 'art': 809,\n",
       " 'period': 810,\n",
       " 'intend': 811,\n",
       " 'interested': 812,\n",
       " 'situation': 813,\n",
       " 'pale': 814,\n",
       " 'accept': 815,\n",
       " 'heaven': 816,\n",
       " 'design': 817,\n",
       " 'd——': 818,\n",
       " 'surface': 819,\n",
       " 'principle': 820,\n",
       " 'meant': 821,\n",
       " 'opportunity': 822,\n",
       " 'intention': 823,\n",
       " 'tomb': 824,\n",
       " 'henry': 825,\n",
       " 'tall': 826,\n",
       " 'wi': 827,\n",
       " 'sing': 828,\n",
       " 'glad': 829,\n",
       " 'nigger': 830,\n",
       " 'shook': 831,\n",
       " 'clock': 832,\n",
       " 'stir': 833,\n",
       " 'spread': 834,\n",
       " 'sorry': 835,\n",
       " 'iron': 836,\n",
       " 'quarter': 837,\n",
       " 'fifty': 838,\n",
       " 'drove': 839,\n",
       " 'handle': 840,\n",
       " 'mud': 841,\n",
       " 'yes': 842,\n",
       " 'notion': 843,\n",
       " 'beg': 844,\n",
       " 'bore': 845,\n",
       " 'tongue': 846,\n",
       " 'discover': 847,\n",
       " 'music': 848,\n",
       " 'dread': 849,\n",
       " 'glass': 850,\n",
       " 'wise': 851,\n",
       " 'public': 852,\n",
       " 'conscience': 853,\n",
       " 'cloud': 854,\n",
       " 'german': 855,\n",
       " 'afraid': 856,\n",
       " 'forth': 857,\n",
       " 'grace': 858,\n",
       " 'doctor': 859,\n",
       " 'trace': 860,\n",
       " 'paid': 861,\n",
       " 'food': 862,\n",
       " 'occur': 863,\n",
       " 'common': 864,\n",
       " 'impossible': 865,\n",
       " 'dozen': 866,\n",
       " 'merlin': 867,\n",
       " 'dry': 868,\n",
       " 'escape': 869,\n",
       " 'ought': 870,\n",
       " 'seize': 871,\n",
       " 'plain': 872,\n",
       " 'aid': 873,\n",
       " 'private': 874,\n",
       " 'number': 875,\n",
       " 'weak': 876,\n",
       " 'author': 877,\n",
       " 'excellent': 878,\n",
       " 'break': 879,\n",
       " 'surprise': 880,\n",
       " 'member': 881,\n",
       " 'mist': 882,\n",
       " 'colour': 883,\n",
       " 'mistress': 884,\n",
       " 'lyford': 885,\n",
       " 'cairo': 886,\n",
       " 'smoke': 887,\n",
       " 'grave': 888,\n",
       " 'whenever': 889,\n",
       " 'throat': 890,\n",
       " 'bury': 891,\n",
       " 'lot': 892,\n",
       " 'difference': 893,\n",
       " 'key': 894,\n",
       " 'guess': 895,\n",
       " 'reader': 896,\n",
       " 'shake': 897,\n",
       " 'energy': 898,\n",
       " 'examine': 899,\n",
       " 'drive': 900,\n",
       " 'field': 901,\n",
       " 'prisoner': 902,\n",
       " 'nearer': 903,\n",
       " 'wander': 904,\n",
       " 'accustom': 905,\n",
       " 'stream': 906,\n",
       " 'gain': 907,\n",
       " 'system': 908,\n",
       " 'forget': 909,\n",
       " 'proceed': 910,\n",
       " 'adventure': 911,\n",
       " 'wheel': 912,\n",
       " 'apparently': 913,\n",
       " 'mail': 914,\n",
       " 'seek': 915,\n",
       " 'fly': 916,\n",
       " 'date': 917,\n",
       " 'travel': 918,\n",
       " 'today': 919,\n",
       " 'shadow': 920,\n",
       " 'sake': 921,\n",
       " 'sky': 922,\n",
       " 'clearly': 923,\n",
       " 'driven': 924,\n",
       " 'distance': 925,\n",
       " 'article': 926,\n",
       " 'example': 927,\n",
       " 'description': 928,\n",
       " 'volume': 929,\n",
       " 'finish': 930,\n",
       " 'medical': 931,\n",
       " 'suggest': 932,\n",
       " 'decide': 933,\n",
       " 'usually': 934,\n",
       " 'beneath': 935,\n",
       " 'perceive': 936,\n",
       " 'visitor': 937,\n",
       " 'vessel': 938,\n",
       " 'supply': 939,\n",
       " 'teach': 940,\n",
       " 'respectable': 941,\n",
       " 'dignity': 942,\n",
       " 'twice': 943,\n",
       " 'fond': 944,\n",
       " 'honour': 945,\n",
       " 'stelling': 946,\n",
       " 'straight': 947,\n",
       " 'considerable': 948,\n",
       " 'confidence': 949,\n",
       " 'damp': 950,\n",
       " 'belong': 951,\n",
       " 'pretend': 952,\n",
       " 'charge': 953,\n",
       " 'gun': 954,\n",
       " 'spite': 955,\n",
       " 'drink': 956,\n",
       " 'dreadful': 957,\n",
       " 'chin': 958,\n",
       " 'rattle': 959,\n",
       " 'judgment': 960,\n",
       " 'value': 961,\n",
       " 'summer': 962,\n",
       " 'ate': 963,\n",
       " 'cat': 964,\n",
       " 'existence': 965,\n",
       " 'repeat': 966,\n",
       " 'generally': 967,\n",
       " 'twenty': 968,\n",
       " 'march': 969,\n",
       " 'mary': 970,\n",
       " 'temper': 971,\n",
       " 'mental': 972,\n",
       " 'nay': 973,\n",
       " 'grass': 974,\n",
       " 'merit': 975,\n",
       " 'noble': 976,\n",
       " 'century': 977,\n",
       " 'immediate': 978,\n",
       " 'render': 979,\n",
       " 'single': 980,\n",
       " 'christian': 981,\n",
       " 'countenance': 982,\n",
       " 'ship': 983,\n",
       " 'convince': 984,\n",
       " 'aware': 985,\n",
       " 'degree': 986,\n",
       " 'amount': 987,\n",
       " 'push': 988,\n",
       " 'propose': 989,\n",
       " 'personage': 990,\n",
       " 'butter': 991,\n",
       " 'hotel': 992,\n",
       " 'building': 993,\n",
       " 'apply': 994,\n",
       " 'broken': 995,\n",
       " 'admit': 996,\n",
       " 'basket': 997,\n",
       " 'skin': 998,\n",
       " 'bottle': 999,\n",
       " 'talent': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTxuijiE_-yM",
    "outputId": "8bca5b83-eb24-4844-a5d9-fd9e3e5685ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 300, 699, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(\n",
    "    sequences, label, test_size=0.3, random_state=42)\n",
    "trainX = np.array(trainX)\n",
    "testX = np.array(testX)\n",
    "trainY = np.array(trainY)\n",
    "testY = np.array(testY)\n",
    "\n",
    "\n",
    "len(trainX), len(testX), len(trainY), len(testY)\n",
    "\n",
    "# trainX.shape, testX.shape, trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3ykWyt8AQHn",
    "outputId": "18819044-db50-4b0b-e8e0-dfe0b3dff242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 100)\n",
      "(300, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 100\n",
    "#transforms a list (of length num_samples) of sequences (lists of integers) \n",
    "#into a 2D Numpy array of shape (num_samples, num_timesteps) num_timesteps is the maxlen argument.\n",
    "\n",
    "train_X_pad = pad_sequences(trainX, maxlen = max_len, dtype='int32')\n",
    "test_X_pad = pad_sequences(testX, maxlen = max_len, dtype='int32')\n",
    "\n",
    "print(train_X_pad.shape)\n",
    "print(test_X_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePA_jmq1Ah-3",
    "outputId": "2bda62a7-30cb-4e17-bf5a-026ec0190ee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "luSRMsQ7AX-7"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "def generate_model(vocab_size, max_len, embedding_size):\n",
    "\n",
    "    _input = Input(max_len)\n",
    "\n",
    "    x = Embedding(input_dim = vocab_size, output_dim = embedding_size) (_input)\n",
    "\n",
    "    x = LSTM(50)(x)\n",
    "\n",
    "    output = Dense(7, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs= [_input], outputs = [output])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "RakgypQwtqr3"
   },
   "outputs": [],
   "source": [
    "#trainY\n",
    "#dropout sloj posle LSTM, moze da se smeni embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3WapLTSAcxF",
    "outputId": "c929db55-7247-4b32-c6db-86e14ca048b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_9 (Embedding)     (None, 100, 80)           959680    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 50)                26200     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 357       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,237\n",
      "Trainable params: 986,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = generate_model(vocab_size , max_len , embedding_size=80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "s3aWV_dmqeUw"
   },
   "outputs": [],
   "source": [
    "trainX= np.array(trainX)\n",
    "trainY= np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9xrX7gwAmZV",
    "outputId": "cdac76ea-9269-486f-e670-8f81a83f03a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 4s 144ms/step - loss: 1.9392 - accuracy: 0.1946 - val_loss: 1.9322 - val_accuracy: 0.2167\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.8902 - accuracy: 0.2389 - val_loss: 1.8975 - val_accuracy: 0.2167\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 1.8097 - accuracy: 0.2418 - val_loss: 1.8608 - val_accuracy: 0.2267\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 1.6176 - accuracy: 0.2647 - val_loss: 1.7171 - val_accuracy: 0.2367\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.3041 - accuracy: 0.6767 - val_loss: 1.6057 - val_accuracy: 0.3833\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 1.0475 - accuracy: 0.7897 - val_loss: 1.3873 - val_accuracy: 0.4867\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.8296 - accuracy: 0.8298 - val_loss: 1.4004 - val_accuracy: 0.4800\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6387 - accuracy: 0.8755 - val_loss: 1.1708 - val_accuracy: 0.6100\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.4884 - accuracy: 0.9399 - val_loss: 1.2387 - val_accuracy: 0.5433\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.4013 - accuracy: 0.9399 - val_loss: 1.0613 - val_accuracy: 0.6100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X_pad, trainY, epochs=10, batch_size=60, validation_data=(test_X_pad, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMmTww8hAqC0",
    "outputId": "1981dc4c-682a-4f20-e0bb-79731ff06513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.17        30\n",
      "           1       0.50      0.56      0.53        39\n",
      "           2       0.48      0.74      0.58        31\n",
      "           3       0.77      0.82      0.79        65\n",
      "           4       0.66      0.85      0.74        46\n",
      "           5       0.59      0.62      0.61        48\n",
      "           6       0.57      0.32      0.41        41\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.58      0.57      0.55       300\n",
      "weighted avg       0.60      0.61      0.58       300\n",
      "\n",
      "[[ 3  1 16  1  4  5  0]\n",
      " [ 0 22  0  3  4  2  8]\n",
      " [ 0  0 23  1  2  5  0]\n",
      " [ 0  4  2 53  0  6  0]\n",
      " [ 0  2  0  3 39  0  2]\n",
      " [ 3  1  7  6  1 30  0]\n",
      " [ 0 14  0  2  9  3 13]]\n",
      "Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(test_X_pad)\n",
    "pred_test = np.argmax(pred_test,axis=1)\n",
    "#pred_test = pred_test.round()\n",
    "\n",
    "#print evaluation metrics \n",
    "print(classification_report(testY,pred_test))\n",
    "print(confusion_matrix(testY,pred_test))\n",
    "print(\"Accuracy:\",accuracy_score(testY, pred_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
